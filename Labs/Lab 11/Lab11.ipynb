{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 11: Unsupervised learning\n",
    "- **Author:** Emily Aiken ([emilyaiken@berkeley.edu](mailto:emilyaiken@berkeley.edu)), based on past labs by Guanghua Chi and Dmitrius Papadimitriou\n",
    "- **Date:** April 13, 2022\n",
    "- **Course:** INFO 251: Applied Machine Learning\n",
    "\n",
    "## Topics:\n",
    "1. K-means clustering\n",
    "2. Principal components analysis with tabular data\n",
    "2. Principal components analysis with image data\n",
    "\n",
    "## Learning Goals:\n",
    "At the end of this lab you will...\n",
    "- Understand the k-means expectation maximization algorithm\n",
    "- Know how to tune the number of clusters for k-means\n",
    "- Know how to fit k-means and PCA with scikit-learn for tabluar and image data\n",
    "- Know how to calculate explained variance and its relationship to the number of components for PCA \n",
    "- Understand how pre-processing with PCA can reduce overfitting in supervised learning settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: k-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs, load_boston, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate isotropic Gaussian blobs for clustering.\n",
    "# See: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html\n",
    "x, y = make_blobs(n_samples=300, n_features=2, centers=4, cluster_std=1, random_state=0)\n",
    "\n",
    "# Plot synthetic data\n",
    "fig, ax = plt.subplots(1, figsize=(6, 5))\n",
    "ax.scatter(x[:, 0], x[:, 1], c=y, s=50, cmap='magma')\n",
    "\n",
    "# Clean up plot\n",
    "ax.set_title('Synthetic Data with True Cluster Labels', fontsize='xx-large')\n",
    "ax.set_xlabel('Feature 1', fontsize='x-large')\n",
    "ax.set_ylabel('Feature 2', fontsize='x-large')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Scikit-learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up subplots\n",
    "fig, ax = plt.subplots(2, 3, figsize=(15, 9), sharey=True, sharex=True)\n",
    "ax=ax.flatten()\n",
    "\n",
    "# Run k-means for different numbers of clusters\n",
    "for n_clusters in range(1, 7):\n",
    "    \n",
    "    # Train and predict with k-means\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(x)\n",
    "    yhat = kmeans.predict(x)\n",
    "    \n",
    "    # Plot data in subplot\n",
    "    ax[n_clusters-1].scatter(x[:, 0], x[:, 1], c=yhat, s=50, cmap='viridis', alpha=0.5)\n",
    "    \n",
    "    # Get and plot cluster centers\n",
    "    centers = kmeans.cluster_centers_\n",
    "    ax[n_clusters-1].scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.7)\n",
    "    \n",
    "    # Clean up subplot\n",
    "    ax[n_clusters-1].set_title('%i Clusters' % n_clusters, fontsize='xx-large')\n",
    "    \n",
    "# Clean up plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. How many clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = np.arange(1, 40)\n",
    "sse = []\n",
    "for n in n_clusters:\n",
    "    kmeans = KMeans(n_clusters=n)\n",
    "    kmeans.fit(x)\n",
    "    sse.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10, 6))\n",
    "ax.plot(n_clusters, sse)\n",
    "ax.set_xlabel('Number of Clusters')\n",
    "ax.set_ylabel('SSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Principal Component Analysis with Tabular Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Load Boston Housing Prices data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load boston housing prices data\n",
    "bdata = load_boston()\n",
    "df = pd.DataFrame(bdata.data)\n",
    "df.columns = bdata.feature_names[:]\n",
    "df['target'] = bdata.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "for col in df.columns:\n",
    "    if col != 'target':\n",
    "        mean, std = df[col].mean(), df[col].std()\n",
    "        df[col] = (df[col] - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test\n",
    "train, test = train_test_split(df, shuffle=True, test_size=0.25, random_state=0)\n",
    "x_train, y_train = train.drop('target', axis=1), train['target']\n",
    "x_test, y_test = test.drop('target', axis=1), test['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Scikit-learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and \"predict\" (transform) with PCA\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(x_train)\n",
    "embedding_train = pca.transform(x_train)\n",
    "embedding_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get % of variance explained\n",
    "print('Share of variation explained by each component: ', pca.explained_variance_ratio_)\n",
    "print('Total variation explained: %.2f' % np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record variation explained based on number of components\n",
    "variation_explained = []\n",
    "n_components = range(1, len(x_train.columns))\n",
    "for n in n_components:\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(x_train)\n",
    "    variation_explained.append(np.sum(pca.explained_variance_ratio_))\n",
    "    \n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, figsize=(10, 6))\n",
    "ax.plot(n_components, variation_explained)\n",
    "ax.set_title('Total Variation Explained by Number of PCA Components', fontsize='xx-large')\n",
    "ax.set_xlabel('Number of Components', fontsize='x-large')\n",
    "ax.set_ylabel('% Variation Explained', fontsize='x-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. PCA and regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try fitting PCAs of k components between 1 and 12 on the training data (pca.fit()). Then, project the \n",
    "# training and test data onto the k-dimensional subspace (pca.transform()). For each PCA transformation, fit an \n",
    "# OLS regression on the projected training features using scikit-learn's LinearRegression() class. Report the r2\n",
    "# score for the regression on the training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: PCA with Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "digits = load_digits()\n",
    "x = digits.images\n",
    "y = digits.target\n",
    "\n",
    "# Standardize data\n",
    "x = (x - np.mean(x, axis=0))/(np.std(x, axis=0) + 0.000000001)\n",
    "\n",
    "# Flatten x data\n",
    "x = x.reshape(x.shape[0], x.shape[1]*x.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. PCA to visualize latent relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform to 2 dimensions with PCA\n",
    "pca = PCA(2)\n",
    "projected = pca.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(projected[:, 0], projected[:, 1], c=y, cmap=plt.cm.get_cmap('Spectral', 10), edgecolor='none', \n",
    "            alpha=0.5)\n",
    "plt.xlabel('Component 1', fontsize='x-large')\n",
    "plt.ylabel('Component 2', fontsize='x-large')\n",
    "plt.title('2 PCA Components for MNIST Images', fontsize='xx-large')\n",
    "\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
