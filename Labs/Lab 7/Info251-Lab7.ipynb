{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7 - Trees, Forests, and Classification\n",
    "- **Author:** Emily Aiken ([emilyaiken@berkeley.edu](mailto:emilyaiken@berkeley.edu))\n",
    "- **Date:** March 9, 2022\n",
    "- **Course:** INFO 251: Applied machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics:\n",
    "1. Decision Trees\n",
    "2. Random Forests\n",
    "3. Classification performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals:\n",
    "At the end of this lab, you will...\n",
    "- Understand the recursive algorithm to grow a decision tree\n",
    "- Know the standard splitting criteria used for decision trees\n",
    "- Understand how feature importances are calculated for decision trees\n",
    "- Know the main hyperparameters for decision trees and random forests, and how to tune them to prevent overfitting\n",
    "- Know the main performance measures for classification: accuracy, TPR, FPR, precision, and recall\n",
    "- Understand ROC curves, precision and recall plots, and methods for determining the \"optimal\" classification threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "- [Feature importances in random forests](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "np.random.seed(11)\n",
    "data = datasets.load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "for col in df.columns:\n",
    "    if col != 'target':\n",
    "        df[col] = df[col] + np.random.normal(0, 4*df[col].std(), len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Observations in class 0: %i' % len(df[df['target'] == 0]))\n",
    "print('Observations in class 1: %i' % len(df[df['target'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test\n",
    "train, test = train_test_split(df, shuffle=True, test_size=0.25, random_state=0)\n",
    "x_train, y_train = train.drop('target', axis=1), train['target']\n",
    "x_test, y_test = test.drop('target', axis=1), test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model without tuning hyperparameters\n",
    "model = DecisionTreeClassifier(max_depth=2)\n",
    "model.fit(x_train, y_train)\n",
    "yhat_train = model.predict(x_train)\n",
    "yhat_test = model.predict(x_test)\n",
    "\n",
    "print('Accuracy (train): %.2f' % accuracy_score(y_train, yhat_train))\n",
    "print('Accuracy (test): %.2f' % accuracy_score(y_test, yhat_test))\n",
    "\n",
    "print('Precision (train): %.2f' % recall_score(y_train, yhat_train))\n",
    "print('Precision (test): %.2f' % recall_score(y_test, yhat_test))\n",
    "\n",
    "print('Recall (train): %.2f' % precision_score(y_train, yhat_train))\n",
    "print('Recall (test): %.2f' % precision_score(y_test, yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameter: max_depth\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "params = {'max_depth':[2, 4, 6, 8, 10, 12, 14]}\n",
    "cv_model = GridSearchCV(model, param_grid=params, scoring='accuracy', refit=True, return_train_score=True, cv=cv)\n",
    "cv_model.fit(x_train, y_train)\n",
    "cv_results = pd.DataFrame(cv_model.cv_results_)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CV accuracy as a function of maximum depth\n",
    "sns.set(font_scale=1.5)\n",
    "fig, ax = plt.subplots(1, figsize=(10, 5))\n",
    "ax.plot(cv_results['param_max_depth'], cv_results['mean_train_score'], label='Train')\n",
    "ax.plot(cv_results['param_max_depth'], cv_results['mean_test_score'], label='Test')\n",
    "ax.set_xlabel('Maximum Depth of Decision Tree')\n",
    "ax.set_ylabel('Average CV Accuracy')\n",
    "ax.set_title('CV Accuracy vs. Tree Depth')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set using best model\n",
    "model = cv_model.best_estimator_\n",
    "model.fit(x_train, y_train)\n",
    "yhat_train = model.predict(x_train)\n",
    "yhat_test = model.predict(x_test)\n",
    "\n",
    "print('Best maximum depth: %i' % cv_model.best_params_['max_depth'])\n",
    "\n",
    "print('Accuracy (train): %.2f' % accuracy_score(y_train, yhat_train))\n",
    "print('Accuracy (test): %.2f' % accuracy_score(y_test, yhat_test))\n",
    "\n",
    "print('Precision (train): %.2f' % recall_score(y_train, yhat_train))\n",
    "print('Precision (test): %.2f' % recall_score(y_test, yhat_test))\n",
    "\n",
    "print('Recall (train): %.2f' % precision_score(y_train, yhat_train))\n",
    "print('Recall (test): %.2f' % precision_score(y_test, yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "importances = pd.DataFrame([x_train.columns, importances]).T\n",
    "importances.columns = ['Feature', 'Importance']\n",
    "importances = importances.sort_values('Importance', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart\n",
    "fig, ax = plt.subplots(1, figsize=(10, 6))\n",
    "plt.barh(importances['Feature'], importances['Importance'])\n",
    "ax.set_xlabel('Gini Importance')\n",
    "ax.set_title('Feature Importances in Decision Tree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model without tuning hyperparameters\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "model.fit(x_train, y_train)\n",
    "yhat_train = model.predict(x_train)\n",
    "yhat_test = model.predict(x_test)\n",
    "\n",
    "print('Accuracy (train): %.2f' % accuracy_score(y_train, yhat_train))\n",
    "print('Accuracy (test): %.2f' % accuracy_score(y_test, yhat_test))\n",
    "\n",
    "print('Precision (train): %.2f' % recall_score(y_train, yhat_train))\n",
    "print('Precision (test): %.2f' % recall_score(y_test, yhat_test))\n",
    "\n",
    "print('Recall (train): %.2f' % precision_score(y_train, yhat_train))\n",
    "print('Recall (test): %.2f' % precision_score(y_test, yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters: max_depth, n_estimators\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "params = {'max_depth':[2, 4, 6, 8], 'n_estimators':[25, 50, 100]}\n",
    "cv_model = GridSearchCV(model, param_grid=params, scoring='accuracy', refit=True, return_train_score=True, cv=cv)\n",
    "cv_model.fit(x_train, y_train)\n",
    "model = cv_model.best_estimator_\n",
    "model.fit(x_train, y_train)\n",
    "yhat_train = model.predict(x_train)\n",
    "yhat_test = model.predict(x_test)\n",
    "\n",
    "print('Best maximum depth: %i' % cv_model.best_params_['max_depth'])\n",
    "print('Best number of estimators: %i' % cv_model.best_params_['n_estimators'])\n",
    "\n",
    "print('Accuracy (train): %.2f' % accuracy_score(y_train, yhat_train))\n",
    "print('Accuracy (test): %.2f' % accuracy_score(y_test, yhat_test))\n",
    "\n",
    "print('Precision (train): %.2f' % recall_score(y_train, yhat_train))\n",
    "print('Precision (test): %.2f' % recall_score(y_test, yhat_test))\n",
    "\n",
    "print('Recall (train): %.2f' % precision_score(y_train, yhat_train))\n",
    "print('Recall (test): %.2f' % precision_score(y_test, yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "yhat_test_proba = model.predict_proba(x_test)[:, 1]\n",
    "fprs, tprs, thresholds = roc_curve(y_test, yhat_test_proba)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(10, 5))\n",
    "ax.scatter(fprs, tprs)\n",
    "ax.plot(fprs, tprs)\n",
    "ax.plot([0, 1], [0, 1], color='grey', dashes=[2, 2])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate (Recall)')\n",
    "ax.set_title('ROC Curve (Test Data)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get area under the curve\n",
    "print('AUC score: %.2f' % roc_auc_score(y_test, yhat_test_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get \"optimal\" threshold: the one closest to the top-left corner of the ROC graph\n",
    "distances_from_top_left = [np.sqrt(tprs[i]**2 + (1-fprs[i])**2) for i in range(len(tprs))]\n",
    "best_cutoff = np.argmin(distances_from_top_left)\n",
    "print('Threshold closest to top-left corner of graph: %.2f (%.2f TPR, %.2f FPR)' % \n",
    "      (thresholds[best_cutoff], tprs[best_cutoff], fprs[best_cutoff]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Precision and Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision vs. recall plot\n",
    "thresholds = np.arange(0, 0.80, .01)\n",
    "precisions = [precision_score(y_test, (yhat_test_proba > t)) for t in thresholds]\n",
    "recalls = [recall_score(y_test, (yhat_test_proba > t)) for t in thresholds]\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(10, 5))\n",
    "ax.plot(thresholds, precisions, label='Precision')\n",
    "ax.plot(thresholds, recalls, label='Recall')\n",
    "ax.set_xlabel('Classification Threshold')\n",
    "ax.set_ylabel('Precision and Recall')\n",
    "ax.set_title('Precision and Recall vs. Classification Threshold')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get \"optimal threshold\": The one where precision and recall are balanced\n",
    "best_threshold = np.argmin(np.abs(np.array(precisions) - np.array(recalls)))\n",
    "print('Best threshold: %.2f (%.2f precision, %.2f recall)' % \n",
    "      (thresholds[best_threshold], precisions[best_threshold], recalls[best_threshold]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "importances = pd.DataFrame([x_train.columns, importances]).T\n",
    "importances.columns = ['Feature', 'Importance']\n",
    "importances = importances.sort_values('Importance', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart\n",
    "fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "plt.barh(importances['Feature'], importances['Importance'], yerr=std)\n",
    "ax.set_xlabel('Mean Decrease in Impurity')\n",
    "ax.set_title('Feature Importances in Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Over to you!\n",
    "New dataset: Boston housing prices\n",
    "- Train a decision tree, try tuning *maximum depth* and report best r2\n",
    "- Train a random forest, try tuning *maximum depth*, *n_estimators* and report best r2\n",
    "- Calculate the feature importances for the best random forest, show mean and standard deviation in impurity decrease\n",
    "\n",
    "You should be able to achieve an r2 score of above 0.70 on the test set using your well-tuned random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_boston()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, shuffle=True, test_size=0.25, random_state=0)\n",
    "x_train, y_train = train.drop('target', axis=1), train['target']\n",
    "x_test, y_test = test.drop('target', axis=1), test['target']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
