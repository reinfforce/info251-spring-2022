{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NydJC_ukpxRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckWwG9pdjl7E"
      },
      "source": [
        "\n",
        "This is a fun but challenging problem set. It will test your python skills, as well as your understanding of the material in class and in the readings. Start early and debug often! Some notes:\n",
        "\n",
        "* Part 1 is meant to be easy, so get through it quickly.\n",
        "* Part 2 (especially 2.1) will be difficult, but it is the lynchpin of this problem set to make sure to do it well and understand what you've done. If you find your gradient descent algorithm is taking more than a few minutes to complete, debug more, compare notes with others, and go to the Lab sessions (especially the sections on vectorized computation and computational efficiency).\n",
        "* Depending on how well you've done 2.1, parts 2.3 and 4.3 will be relatively painless or incredibly painful. \n",
        "\n",
        "* Part 4 (especially 4.3) will be computationally intensive. Don't leave this until the last minute, otherwise your code might be running when the deadline arrives.\n",
        "\n",
        "* Do the extra credit problems last. This can help you increase your scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Bscj7wWjl7I"
      },
      "source": [
        "---\n",
        "\n",
        "## Introduction to the assignment\n",
        "\n",
        "As with the last assignment, you will be using the [Boston Housing Prices Data Set](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "t0_LwTzXjl7J"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import sklearn\n",
        "\n",
        "%matplotlib inline  \n",
        "import matplotlib.pyplot as plt  \n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import statsmodels.formula.api as smf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "72miT9Zhjl7K"
      },
      "outputs": [],
      "source": [
        "# Load you data the Boston Housing data into a dataframe\n",
        "# MEDV.txt containt the median house values and data.txt the other 13 features\n",
        "# in order [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\"]\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cbXmKF9jl7L"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 1: Getting oriented\n",
        "\n",
        "\n",
        "### 1.1 Use existing libraries\n",
        "\n",
        "Soon, you will write your own gradient descent algorithm, which you will then use to minimize the squared error cost function.  First, however, let's use the canned versions that come with Python, to make sure we understand what we're aiming to achieve.\n",
        "\n",
        "Using the same Boston housing prices dataset, use the [Linear Regression class](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from sklearn or the [OLS class](http://wiki.scipy.org/Cookbook/OLS) from SciPy to explore the relationship between median housing price and number of rooms per house. Do the following:\n",
        "\n",
        "(a) Regress the housing price on the number of rooms per house. Draw a scatter plot of housing price (y-axis) against rooms (x-axis), and draw the regression line in blue.  You might want to make the dots semi-transparent if it improves the presentation of the figure. \n",
        "\n",
        "(b) Regress the housing price on the number of rooms per house and the (number of rooms per house) squared.  Show the (curved) regression line in green. \n",
        "\n",
        "(c) Interpret your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hxCNK0pTjl7L"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjxD1XjXjl7M"
      },
      "source": [
        "*Enter your observations here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkO8pHRvjl7N"
      },
      "source": [
        "### 1.2 Training and testing\n",
        "\n",
        "Chances are, for the above problem you used all of your data to fit the regression line. In some circumstances this is a reasonable thing to do, but often this will result in overfitting. Let's redo the above results the ML way, using careful cross-validation.  Since you are now experts in cross-validation, and have written your own cross-validation algorithm from scratch, you can now take a shortcut and use the libraries that others have built for you.\n",
        "\n",
        "Using the [cross-validation functions](http://scikit-learn.org/stable/modules/cross_validation.html) from scikit-learn, use 5-fold cross-validation to fit the regression model (a) from 1.1, i.e. the linear fit of housing price on number of rooms per house. Each fold of cross-validation will give you one slope coefficient and one intercept coefficient.  Create a new scatterplot of housing price against rooms, and draw the five different regression lines in light blue, and the oroginal regression line from 1.1 in red (which was estimated using the full dataset). What do you notice?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gnVo_0TDjl7O"
      },
      "outputs": [],
      "source": [
        "from sklearn.cross_validation import KFold\n",
        "\n",
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9CI66kKjl7O"
      },
      "source": [
        "*Enter your observations here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cblgmgiBjl7P"
      },
      "source": [
        "## Part 2: Gradient descent: Linear Regression\n",
        "\n",
        "This is where it gets fun!\n",
        "\n",
        "### 2.1 Implement gradient descent with one independent variable (average rooms per house)\n",
        "\n",
        "Implement the batch gradient descent algorithm that we discussed in class. Use the version you implement to regress the housing price on the number of rooms per house. Experiment with 3-4 different values of the learning rate *R*, and do the following:\n",
        "\n",
        "* Report the values of alpha and beta that minimize the loss function\n",
        "* Report the number of iterations it takes for your algorithm to converge (for each value of *R*)\n",
        "* Report the total running time of your algorithm, in seconds\n",
        "* How do your coefficients compare to the ones estimated through standard libraries? Does this depend on *R*?\n",
        "\n",
        "Some skeleton code is provided below, but you should feel free to delete this code and start from scratch if you prefer.\n",
        "\n",
        "* *Hint 1: Don't forget to implement a stopping condition, so that at every iteration you check whether your results have converged. Common approaches to this are to (a) check to see if the loss has stopped decreasing; and (b) check if both your current parameter esimates are close to the estimates from the previous iteration.  In both cases, \"close\" should not be ==0, it should be <=epsilon, where epsilon is something very small (like 0.0001).*\n",
        "* *Hint 2: Some people like to include a MaxIterations parameter in their gradient descent algorithm, to prevent divergence. *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yAfAQ5EQjl7Q"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "\"\"\"\n",
        "Function\n",
        "--------\n",
        "bivariate_ols\n",
        "    Gradient Decent to minimize OLS. Used to find co-efficients of bivariate OLS Linear regression\n",
        "\n",
        "Parameters\n",
        "----------\n",
        "xvalues, yvalues : narray\n",
        "    xvalues: independent variable\n",
        "    yvalues: dependent variable\n",
        "    \n",
        "R: float\n",
        "    Learning rate\n",
        "    \n",
        "MaxIterations: Int\n",
        "    maximum number of iterations\n",
        "    \n",
        "\n",
        "Returns\n",
        "-------\n",
        "alpha: float\n",
        "    intercept\n",
        "    \n",
        "beta: float\n",
        "    co-efficient\n",
        "\"\"\"\n",
        "def bivariate_ols(xvalues, yvalues, R=0.01, MaxIterations=1000):\n",
        "    # initialize the parameters\n",
        "    start_time = time.time()\n",
        "\n",
        "        # your code here\n",
        "\n",
        "    print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))\n",
        "    return alpha, beta\n",
        "\n",
        "# example function call\n",
        "# print(bivariate_ols(X, Y, 0.01, 100000))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7dK9bMZjl7Q"
      },
      "source": [
        "*Enter your observations here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZqg5yW3jl7Q"
      },
      "source": [
        "### 2.2 Data normalization (done for you!)\n",
        "\n",
        "Soon, you will implement a version of gradient descent that can use an arbitrary number of independent variables. Before doing this, we want to give you some code in case you want to standardize your features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lU9-kG3Tjl7Q"
      },
      "outputs": [],
      "source": [
        "def standardize(raw_data):\n",
        "    return ((raw_data - np.mean(raw_data, axis = 0)) / np.std(raw_data, axis = 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUUackd0jl7R"
      },
      "source": [
        "### 2.3 Implement gradient descent with an arbitrary number of independent variables\n",
        "\n",
        "Now that you have a simple version of gradient descent working, create a version of gradient descent that can take more than one independent variable.  Assume all independent variables will be continuous.  Test your algorithm using TAX and RM as independent variables. Standardize these variables before inputting them to the gradient descent algorithm. \n",
        "\n",
        "As before,  report and interpret your estimated coefficients, the number of iterations before convergence, and the total running time of your algorithm. Experiment with 2-3 different values of R.\n",
        "\n",
        "* *Hint 1: Be careful to implement this efficiently, otherwise it might take a long time for your code to run. Commands like `np.dot` can be a good friend to you on this problem*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yhEcthNtjl7R"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Function\n",
        "--------\n",
        "multivariate_ols\n",
        "    Gradient Decent to minimize OLS. Used to find co-efficients of bivariate OLS Linear regression\n",
        "\n",
        "Parameters\n",
        "----------\n",
        "xvalue_matrix, yvalues : narray\n",
        "    xvalue_matrix: independent variable\n",
        "    yvalues: dependent variable\n",
        "    \n",
        "R: float\n",
        "    Learning rate\n",
        "    \n",
        "MaxIterations: Int\n",
        "    maximum number of iterations\n",
        "    \n",
        "\n",
        "Returns\n",
        "-------\n",
        "alpha: float\n",
        "    intercept\n",
        "    \n",
        "beta_array: array[float]\n",
        "    co-efficient\n",
        "\"\"\"\n",
        "\n",
        "def multivariate_ols(xvalue_matrix, yvalues, R=0.01, MaxIterations=1000):\n",
        "    # your code here\n",
        "    return alpha, beta_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2o_SDdcjl7R"
      },
      "source": [
        "*Enter your observations here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMXw_qcYjl7S"
      },
      "source": [
        "### 2.4 Compare standardized vs. non-standardized results\n",
        "\n",
        "Repeat the analysis from 2.3, but this time do not standardize your variables - i.e., use the original data. Use the same three values of R (0.1, 0.01, and 0.001). What do you notice about the running time and convergence properties of your algorithm?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AnEvJxxhjl7S"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFdApjEsjl7S"
      },
      "source": [
        "*Enter your observations here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pEk2_UZjl7S"
      },
      "source": [
        "## 3. Prediction\n",
        "\n",
        "Let's use our fitted model to make predictions about housing prices. Make sure to first standardize your features before proceeding.\n",
        "\n",
        "### 3.1 Cross-Validation\n",
        "\n",
        "Unless you were careful above, you probably overfit your data again. Let's fix that. Use 5-fold cross-validation to re-fit the multivariate regression from 2.3 above, and report your estimated coefficients (there should be three, corresponding to the intercept and the two coefficients for TAX and RM). Since there are 5 folds, there will be 5 sets of three coefficients -- report them all in a 5x3 table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DGQRERuIjl7T"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHk88-z5jl7T"
      },
      "source": [
        "*Discuss your results here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaRVr5wpjl7T"
      },
      "source": [
        "### 3.2 Predicted values and RMSE\n",
        "\n",
        "Let's figure out how accurate this predictive model turned out to be. Compute the cross-validated RMSE for each of the 5 folds above. In other words, in fold 1, use the parameters estimated on the 80% of the data to make predictions for the 20%, and calculate the RMSE for those 20%. Repeate this for the remaining folds. Report the RMSE for each of the 5-folds, and the average (mean) RMSE across the five folds. How does this average RMSE compare to the performance of your nearest neighbor algorithm from the last problem set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9I1zFLBajl7T"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61FuGxiYjl7T"
      },
      "source": [
        "*Discuss your results here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L-AsWOajl7T"
      },
      "source": [
        "### Extra Credit 1: Logistic Regression\n",
        "\n",
        "For extra credit, implement logistic regression using gradient descent. Create a new variable (EXPENSIVE) to indicate whether the median housing price is more than $40,000. Use your model  a logistic regression of EXPENSIVE on CHAS and RM. Report your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UR9qYdndjl7T"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04AamlXLjl7T"
      },
      "source": [
        "*Discuss your results here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvkpICWsjl7T"
      },
      "source": [
        "## 4 Regularization \n",
        "\n",
        "### 4.1 Get prepped\n",
        "\n",
        "Step 1: Create new interaction variables between each possible pair of the F_s features. If you originally had *K* features, you should now have K+(K*(K+1))/2 features. Standardize all of your features.\n",
        "\n",
        "Step 2: Randomly sample 80% of your data and call this the training set, and set aside the remaining 20% as your test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "89-Zqpssjl7U"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeXkvp7njl7U"
      },
      "source": [
        "### 4.2 Overfitting (sort of)\n",
        "Now, using your version of multivariate regression from 2.3, let's overfit the training data. Using your training set, regress housing price on as many of those K+(K*(K+1))/2 features as you can (Don't forget to add quadratic terms. Form instance, RM^2.).  If you get too greedy, it's possible this will take a long time to compute, so start with 5-10 features, and if you have the time, add more features.\n",
        "\n",
        "Report the RMSE when you apply your model to your training set and to your testing set. How do these numbers compare to each other, and to the RMSE from 3.2 and nearest neighbors?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kgQ48hi6jl7U"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf3h2JEIjl7U"
      },
      "source": [
        "*Discuss your results here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDlDsi4wjl7U"
      },
      "source": [
        "### 4.3 Ridge regularization (basic)\n",
        "Incorporate L2 (Ridge) regularization into your multivariate_ols regression. Write a new version of your gradient descent algorithm that includes a regularization term \"lambda\" to penalize excessive complexity. \n",
        "\n",
        "Use your regularized regression to re-fit the model from 4.2 above on your training data, using the value lambda = 0.5.  Report the RMSE obtained for your training data, and the RMSE obtained for your testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Rlaok4imjl7U"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxZdfCKvjl7U"
      },
      "source": [
        "*Discuss your results here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIYjxCfAjl7U"
      },
      "source": [
        "### 4.4: Cross-validate lambda\n",
        "\n",
        "This is where it all comes together! Use k-fold cross-validation to select the optimal value of lambda. In other words, define a set of different values of lambda. Then, using the 80% of your data that you set aside for training, iterate through the values of lambda one at a time. For each value of lambda, use k-fold cross-validation to compute the average cross-validated (test) RMSE for that lambda value, computed as the average across the held-out folds. You should also record the average cross-validated train RMSE, computed as the average across the folds used for training. Create a scatter plot that shows RMSE as a function of lambda. The scatter plot should have two lines: a red line showing the cross-validated (test) RMSE, and a blue line showing the cross-validated train RMSE.  At this point, you should not have touched your held-out 20% of \"true\" test data.\n",
        "\n",
        "What value of lambda minimizes your cross-validated (test) RMSE? Fix that value of lambda, and train a new model using all of your training data with that value of lambda (i.e., use the entire 80% of the data that you set aside in 4.1). Calcuate the RMSE for this model on the 20% of \"true\" test data. How does your test RMSE compare to the RMSE from 4.3, 4.2, 2.3, and to the RMSE from nearest neighbors? What do you make of these results?\n",
        "\n",
        "Go brag to your friends about how you just implemented cross-validated ridge-regularized multivariate regression using gradient descent optimization, from scratch. If you still have friends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "35QDSkeGjl7U"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJRo-C3kjl7U"
      },
      "source": [
        "*Discuss your results here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izzKtl3_jl7U"
      },
      "source": [
        "###  Extra Credit 2: AdaGrad\n",
        "\n",
        "AdaGrad is a method to implement gradient descent with different learning rates for each feature. Adaptive algorithms like this one are being extensively used especially in neural network training. Implement AdaGrad on 2.3 but now use CRIM, RM and DIS as independent variables. Standardize these variables before inputting them to the gradient descent algorithm. Tune the algorithm until you estimate the regression coefficients within a tolerance of 1e-1. Use mini-batch gradient descent in this implementation. In summary for each parameter (in our case one intercept and three slopes) the update step of the gradient (in this example $\\beta_j$) at iteration $k$ of the GD algorithm becomes:\n",
        "\n",
        "$$\\beta_j=\\beta_j -\\frac{R}{\\sqrt{G^{(k)}_j}}\\frac{\\partial J(\\alpha,\\beta_1,\\ldots)}{\\partial \\beta_j}$$ where\n",
        "$G^{(k)}_j=\\sum_{i=1}^{k} (\\frac{\\partial J^{(i)}(\\alpha,\\beta_1,\\ldots)}{\\partial \\beta_j})^2$ and $R$ is your learning rate. The notation $\\frac{\\partial J^{(i)}(\\alpha,\\beta_1,\\ldots)}{\\partial \\beta_j}$ corresponds to the value of the gradient at iteration $(i)$. Essentially we are \"storing\" information about previous iteration gradients. Doing that we effectively decrease the learning rate slower when a feature $x_i$ is sparse (i.e. has many zero values which would lead to zero gradients). Although this method is not necessary for our regression problem, it is good to be familiar with these methods as they are widely used in neural network training.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "n-fxLK0Rjl7V"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAa0A7Zzjl7V"
      },
      "source": [
        "*Discuss your results here*"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "_INFO251_PS4_2022_Feb22.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}